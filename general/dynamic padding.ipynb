{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torchtext\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom collections import defaultdict\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils, datasets\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reviews=['No man is an island','Entire of itself',\n'Every man is a piece of the continent','part of the main',\n'If a clod be washed away by the sea','Europe is the less',\n'As well as if a promontory were','As well as if a manor of thy friend',\n'Or of thine own were','Any manâ€™s death diminishes me',\n'Because I am involved in mankind',\n'And therefore never send to know for whom the bell tolls',\n'It tolls for thee']\n \nlabels=[random.randint(0, 1) for i in range(13)]\n \ndataset=list(zip(reviews,labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self):\n        self.text = reviews\n        \n        #self.text  =  df.review.to_numpy()\n        #self.label =  df.sentiment.to_numpy()\n        \n        self.tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self,idx):\n        \n        encode = self.tokenizer(self.text[idx],\n                                truncation=False,\n                                return_attention_mask = True,\n                                return_token_type_ids=True,\n                                padding=False\n                                )\n        \n        return {\n            \n            'input_ids': encode['input_ids'],\n            'attention_mask': encode['attention_mask'],\n            'token_type_ids': encode['token_type_ids']\n        }\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class collate:\n    \n    def __init__(self,config):\n        \n        self.config = config\n        self.seq_dic = defaultdict(int) \n        self.batch_record = defaultdict(list)\n        self.bn = 0\n        \n    def __call__(self,batch):\n        \n        out = {'input_ids' :[],\n               'attention_mask':[],\n               'token_type_ids':[],\n                'target':[]\n            \n        }\n        \n        for i in batch:\n            for k,v in i.items():\n                out[k].append(v)\n                \n        if self.config['bucket']:\n            max_pad = 0\n            \n            for p in out['input_ids']:\n                if max_pad < len(p):\n                    max_pad = len(p)\n                    \n        else:\n            max_pad = self.config['max_len']\n        \n        \n        #self.batch_record[str(self.bn)] = [len(x) for x in out['input_ids']]  \n        #self.seq_dic[str(self.bn)] = max_pad\n        #self.bn+=1\n        \n        \n        for i in range(len(batch)):\n            input_ids = out['input_ids'][i]\n            attention_mask = out['attention_mask'][i]\n            token_type_ids = out['token_type_ids'][i]\n            \n            str_len = len(input_ids)\n            out['input_ids'][i] = (out['input_ids'][i] +[1]*(max_pad - str_len))[:max_pad]\n            out['attention_mask'][i] = (out['attention_mask'][i] + [0] * (max_pad - str_len))[:max_pad]\n            out['token_type_ids'][i] = (out['token_type_ids'][i] + [0] * (max_pad - str_len))[:max_pad]\n            \n            \n        out['input_ids'] = torch.tensor(out['input_ids'],dtype=torch.long)\n        out['attention_mask'] = torch.tensor(out['attention_mask'],dtype=torch.long)\n        out['token_type_ids'] = torch.tensor(out['token_type_ids'],dtype=torch.long)  \n        return out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'bucket':False,\n    'max_len':12,\n    'batch_size':4\n}\n\ntrain_ds = CLRPDataset()\nsequence = collate(config)\n\ntrain_dataloader = DataLoader(train_ds,\n                              batch_size=config['batch_size'],\n                             collate_fn=sequence,\n                             shuffle=False)\n\n\nfor i,data in enumerate(train_dataloader):\n    for l in data['input_ids']:\n        print(len(l))\n    print('**********')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config = {\n    'bucket':True,\n    'batch_size':4\n}\n\ntrain_ds = CLRPDataset()\nsequence = collate(config)\n\ntrain_dataloader = DataLoader(train_ds,\n                              batch_size=config['batch_size'],\n                             collate_fn=sequence,\n                             shuffle=True)\n\n\nfor i,data in enumerate(train_dataloader):\n    print(data)\n    #for l in data['input_ids']:\n    #    print(len(l))\n    #print('**********')\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}