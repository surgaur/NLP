{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torchtext\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\nfrom collections import defaultdict\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import Counter\nimport torch.nn.functional as F\nfrom torchvision import transforms, utils, datasets\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, WeightedRandomSampler\nfrom torch.utils.data import Dataset, Sampler, DataLoader\nimport operator \nimport os\nfrom sklearn.utils import shuffle\n\nfrom tqdm import tqdm\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:52.152246Z","iopub.execute_input":"2022-01-09T13:08:52.153276Z","iopub.status.idle":"2022-01-09T13:08:55.028706Z","shell.execute_reply.started":"2022-01-09T13:08:52.153147Z","shell.execute_reply":"2022-01-09T13:08:55.027741Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#import tensorflow_datasets as tfds","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.030490Z","iopub.execute_input":"2022-01-09T13:08:55.030742Z","iopub.status.idle":"2022-01-09T13:08:55.034508Z","shell.execute_reply.started":"2022-01-09T13:08:55.030712Z","shell.execute_reply":"2022-01-09T13:08:55.033700Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path = '../input/imdb-dataset-of-50k-movie-reviews'\ndf = pd.read_csv('../input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\ndf = df.head(200)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.035895Z","iopub.execute_input":"2022-01-09T13:08:55.036126Z","iopub.status.idle":"2022-01-09T13:08:55.730667Z","shell.execute_reply.started":"2022-01-09T13:08:55.036091Z","shell.execute_reply":"2022-01-09T13:08:55.729806Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nreviews=['No man is an island itself itself','Entire of itself',\n'Every man is a piece of the continent itself','part of the main',\n'If a clod be washed away by the sea','Europe itself is the less',\n'As well as if a promontory were focus meaning','As well as if a itself manor of thy friend focus focus focus',\n'Or of thine own were meaning','Any manâ€™s death diminishes me meaning',\n'Because I am involved in mankind focus focus island',\n'And therefore never send to know for whom the bell tolls',\n'It tolls for thee','brrrr wrrrr drrrr island']\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.733067Z","iopub.execute_input":"2022-01-09T13:08:55.733972Z","iopub.status.idle":"2022-01-09T13:08:55.738889Z","shell.execute_reply.started":"2022-01-09T13:08:55.733925Z","shell.execute_reply":"2022-01-09T13:08:55.737940Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class tokenizer:\n    \n    def __init__(self,texts, max_features ,max_len , zero_first=False):\n        \n        self.texts = texts\n        self.max_features = max_features\n        self.max_len = max_len\n        self.token2id  = self.buildvocab()\n        #self.tokens = self.tokenize()\n        self.zero_first = zero_first\n\n    def buildvocab(self):\n        counter = Counter()\n        '''\n            UNK - \"unknown token\" - is used to replace the rare words that did not fit in your vocabulary. \n            So your sentence My name is guotong1988 will be translated into My name is _unk_.\n            PAD - your GPU (or CPU at worst) processes your training data in batches and all the sequences in your batch should have the same length. \n            If the max length of your sequence is 8, your sentence .\n            My name is guotong1988 will be padded from either side to fit this length: My name is guotong1988 _pad_ _pad_ _pad_ _pad_\n            ---------------------------------------------------------------------------------------------------------------------------------------\n            Example :: texts = ['i love barcelona' , 'barcelona is in spain','spain is in europe' , 'europe spain barcelona']\n        '''\n\n        vocab = {'<PAD>':0 , '<UNK>':1 }\n        for text in self.texts:\n            counter.update(text.split())\n\n        for idx,(token , count) in enumerate(counter.most_common(self.max_features)):\n            vocab.update({token:idx+2})\n\n        ## Build tokenizer dictionary\n        token2id = {k:v for v,k in enumerate(vocab.keys())}\n        id2token = {v:k for v,k in enumerate(vocab.keys())}\n\n        return token2id \n    \n    def tokenize(self):\n        \n        return  [[(self.token2id.get(token,1)) for token in text.split()[:self.max_len]] for text in self.texts ]\n    \n    def padded_sequence(self , tokens):\n        \n        lens = [len(seq) for seq in tokens]\n        max_len = max(lens)\n        #print('max_len' , max_len)\n        \n        padded_seqs = torch.zeros(len(tokens),max_len).long()\n\n        for idx,seq in enumerate(tokens):\n            if self.zero_first:\n                ok = self.max_len - len(tokens[idx])\n                padded_seqs[idx][ok:len(tokens[idx])+ok] = torch.LongTensor(tokens[idx])\n            else:\n                padded_seqs[idx][:len(tokens[idx])] = torch.LongTensor(tokens[idx])\n\n        return padded_seqs","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.740294Z","iopub.execute_input":"2022-01-09T13:08:55.740656Z","iopub.status.idle":"2022-01-09T13:08:55.758211Z","shell.execute_reply.started":"2022-01-09T13:08:55.740615Z","shell.execute_reply":"2022-01-09T13:08:55.757511Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"texts = df['review'].to_list()\nmax_features = 1500\nmax_len = 250\nt = tokenizer(texts , max_features , max_len)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.759145Z","iopub.execute_input":"2022-01-09T13:08:55.759383Z","iopub.status.idle":"2022-01-09T13:08:55.793857Z","shell.execute_reply.started":"2022-01-09T13:08:55.759350Z","shell.execute_reply":"2022-01-09T13:08:55.793061Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#tokens = t.tokenize()\n#tokens = t.padded_sequence(tokens)\n#index, seqs = zip(*train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.794996Z","iopub.execute_input":"2022-01-09T13:08:55.795833Z","iopub.status.idle":"2022-01-09T13:08:55.799762Z","shell.execute_reply.started":"2022-01-09T13:08:55.795796Z","shell.execute_reply":"2022-01-09T13:08:55.798625Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self , toks , max_len = 250):\n        self.seqs =  toks        \n        self.maxlen = max_len\n        #self.targets =  df['sentiment'].to_list()\n    def __len__(self):\n        return len(self.seqs)\n    \n    def get_keys(self):\n        lens = np.fromiter(\n            tqdm(((min(self.maxlen, len(c))) for c in self.seqs), desc='generate lens'),\n            dtype=np.int32)\n        return lens       \n    \n    def __getitem__(self,idx):\n        return idx,self.seqs[idx]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.801596Z","iopub.execute_input":"2022-01-09T13:08:55.801888Z","iopub.status.idle":"2022-01-09T13:08:55.812413Z","shell.execute_reply.started":"2022-01-09T13:08:55.801850Z","shell.execute_reply":"2022-01-09T13:08:55.811808Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class BucketSampler(Sampler):\n\n    def __init__(self, data_source, sort_keys, bucket_size=None, batch_size=64, shuffle_data=True):\n        super().__init__(data_source)\n        self.shuffle = shuffle_data\n        self.batch_size = batch_size\n        self.sort_keys = sort_keys\n        self.bucket_size = bucket_size if bucket_size is not None else len(sort_keys)\n        self.weights = None\n\n        if not shuffle_data:\n            self.index = self.prepare_buckets()\n        else:\n            self.index = None\n\n    def set_weights(self, weights):\n        assert weights >= 0\n        total = np.sum(weights)\n        if total != 1:\n            weights = weights / total\n        self.weights = weights\n\n    def __iter__(self):\n        indices = None\n        if self.weights is not None:\n            total = len(self.sort_keys)\n            indices = np.random.choice(total, (total,), p=self.weights)\n        if self.shuffle:\n            self.index = self.prepare_buckets(indices)\n        return iter(self.index)\n\n    def get_reverse_indexes(self):\n        indexes = np.zeros((len(self.index),), dtype=np.int32)\n        for i, j in enumerate(self.index):\n            indexes[j] = i\n        return indexes\n\n    def __len__(self):\n        return len(self.sort_keys)\n        \n    def prepare_buckets(self, indices=None):\n        lens = - self.sort_keys\n        assert self.bucket_size % self.batch_size == 0 or self.bucket_size == len(lens)\n\n        if indices is None:\n            if self.shuffle:\n                indices = shuffle(np.arange(len(lens), dtype=np.int32))\n                lens = lens[indices]\n            else:\n                indices = np.arange(len(lens), dtype=np.int32)\n\n        #  bucket iterator\n        def divide_chunks(l, n):\n            if n == len(l):\n                yield np.arange(len(l), dtype=np.int32), l\n            else:\n                # looping till length l\n                for i in range(0, len(l), n):\n                    data = l[i:i + n]\n                    yield np.arange(i, i + len(data), dtype=np.int32), data\n    \n        new_indices = []\n        extra_batch = None\n        for chunk_index, chunk in divide_chunks(lens, self.bucket_size):\n            # sort indices in bucket by descending order of length\n            indices_sorted = chunk_index[np.argsort(chunk, axis=-1)]\n            batches = []\n            for _, batch in divide_chunks(indices_sorted, self.batch_size):\n                if len(batch) == self.batch_size:\n                    batches.append(batch.tolist())\n                else:\n                    assert extra_batch is None\n                    assert batch is not None\n                    extra_batch = batch\n    \n            # shuffling batches within buckets\n            if self.shuffle:\n                batches = shuffle(batches)\n            for batch in batches:\n                new_indices.extend(batch)\n    \n        if extra_batch is not None:\n            new_indices.extend(extra_batch)\n        return indices[new_indices]","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.813892Z","iopub.execute_input":"2022-01-09T13:08:55.815819Z","iopub.status.idle":"2022-01-09T13:08:55.845583Z","shell.execute_reply.started":"2022-01-09T13:08:55.815752Z","shell.execute_reply":"2022-01-09T13:08:55.844866Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def collate_fn(data):\n    index, seqs_ = zip(*data)\n    seqs = t.padded_sequence(seqs_)\n    return index, seqs","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.849713Z","iopub.execute_input":"2022-01-09T13:08:55.849978Z","iopub.status.idle":"2022-01-09T13:08:55.864700Z","shell.execute_reply.started":"2022-01-09T13:08:55.849947Z","shell.execute_reply":"2022-01-09T13:08:55.863714Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#tokens = t.tokenize()\n#tokens = t.padded_sequence(tokens)\n#index, seqs = zip(*train_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:55.866986Z","iopub.execute_input":"2022-01-09T13:08:55.867270Z","iopub.status.idle":"2022-01-09T13:08:55.877569Z","shell.execute_reply.started":"2022-01-09T13:08:55.867234Z","shell.execute_reply":"2022-01-09T13:08:55.876341Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_dataset  = CLRPDataset(t.tokenize() , max_len=max_len)\ntrain_sampler = BucketSampler(train_dataset , train_dataset .get_keys(),\n                                  bucket_size=64, batch_size=64)\ntrain_loader = DataLoader(train_dataset,sampler=train_sampler, batch_size=16, num_workers=0, collate_fn=collate_fn)\nfor i,data in enumerate(train_loader):\n    print(data[1].shape[1])\n    print('********************************')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:08:56.285798Z","iopub.execute_input":"2022-01-09T13:08:56.286320Z","iopub.status.idle":"2022-01-09T13:08:56.321530Z","shell.execute_reply.started":"2022-01-09T13:08:56.286288Z","shell.execute_reply":"2022-01-09T13:08:56.320927Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset  = CLRPDataset(t.tokenize() , max_len=max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, num_workers=0, collate_fn=collate_fn)\nfor i,data in enumerate(train_loader):\n    print(data[1].shape[1])\n    print('********************************')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:13.322368Z","iopub.execute_input":"2022-01-09T13:09:13.322984Z","iopub.status.idle":"2022-01-09T13:09:13.350867Z","shell.execute_reply.started":"2022-01-09T13:09:13.322918Z","shell.execute_reply":"2022-01-09T13:09:13.349857Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def build_vocab(texts, max_features):\n    counter = Counter()\n    for text in texts:\n        counter.update(text.split())\n\n    vocab = {\n        'token2id': {'<PAD>': 0, '<UNK>': max_features + 1},\n        'id2token': {}\n    }\n    vocab['token2id'].update(\n        {token: _id + 1 for _id, (token, count) in\n         enumerate(counter.most_common(max_features))})\n    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n    return vocab\n\n\ndef tokenize(texts, vocab):\n    \n    def text2ids(text, token2id , max_len = max_len):\n        return [\n            token2id.get(token, len(token2id) - 1)\n            for token in text.split()[:max_len]] # \n    \n    return [\n        text2ids(text, vocab['token2id'])\n        for text in texts]\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:18.739249Z","iopub.execute_input":"2022-01-09T13:09:18.739584Z","iopub.status.idle":"2022-01-09T13:09:18.749462Z","shell.execute_reply.started":"2022-01-09T13:09:18.739530Z","shell.execute_reply":"2022-01-09T13:09:18.748827Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vocab = build_vocab(df['review'].to_list(), max_features)\ntrain_x = np.array(tokenize(df['review'].to_list(), vocab))","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:19.369396Z","iopub.execute_input":"2022-01-09T13:09:19.369940Z","iopub.status.idle":"2022-01-09T13:09:19.413543Z","shell.execute_reply.started":"2022-01-09T13:09:19.369904Z","shell.execute_reply":"2022-01-09T13:09:19.412694Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def collate_fn(data):\n\n    def _pad_sequences(seqs):\n        lens = [len(seq) for seq in seqs]\n        max_len = max(lens)\n\n        padded_seqs = torch.zeros(len(seqs), max_len).long()\n        for i, seq in enumerate(seqs):\n            start = max_len - lens[i]\n            padded_seqs[i, start:] = torch.LongTensor(seq)\n        return padded_seqs\n\n    index, seqs = zip(*data)\n    seqs = _pad_sequences(seqs)\n    return index, seqs#, torch.FloatTensor(targets)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:19.716932Z","iopub.execute_input":"2022-01-09T13:09:19.717378Z","iopub.status.idle":"2022-01-09T13:09:19.723564Z","shell.execute_reply.started":"2022-01-09T13:09:19.717338Z","shell.execute_reply":"2022-01-09T13:09:19.722969Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CLRPDataset(Dataset):\n    def __init__(self , seqs,maxlen):\n        self.seqs =  train_x #df['review'].to_list()        \n        self.maxlen = maxlen\n        #self.targets =  df['sentiment'].to_list()\n    \n    def __len__(self):\n        return len(self.seqs)\n    \n    def get_keys(self):\n        lens = np.fromiter(\n            tqdm(((min(self.maxlen, len(c))) for c in self.seqs), desc='generate lens'),\n            dtype=np.int32)\n        return lens\n    \n    def __getitem__(self,idx):\n        return idx,self.seqs[idx]    ","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:20.462972Z","iopub.execute_input":"2022-01-09T13:09:20.463408Z","iopub.status.idle":"2022-01-09T13:09:20.470995Z","shell.execute_reply.started":"2022-01-09T13:09:20.463378Z","shell.execute_reply":"2022-01-09T13:09:20.470132Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"max_len = 250 \ntrain_dataset  = CLRPDataset(train_x , maxlen = max_len)\ntrain_sampler = BucketSampler(train_dataset , train_dataset .get_keys(),\n                                  bucket_size=64, batch_size=64)\ntrain_loader = DataLoader(train_dataset,sampler=train_sampler, batch_size=16, num_workers=0, collate_fn=collate_fn)\nfor i,data in enumerate(train_loader):\n    print(data[1].shape[1])\n    print('********************************')","metadata":{"execution":{"iopub.status.busy":"2022-01-09T13:09:21.205704Z","iopub.execute_input":"2022-01-09T13:09:21.206116Z","iopub.status.idle":"2022-01-09T13:09:21.229797Z","shell.execute_reply.started":"2022-01-09T13:09:21.206086Z","shell.execute_reply":"2022-01-09T13:09:21.228952Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}